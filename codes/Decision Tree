class DecisionTreeNode:
    def __init__(self, feature=None, value=None, left=None, right=None, label=None):
        self.feature = feature
        self.value = value
        self.left = left
        self.right = right
        self.label = label

def build_tree(data, features):
    if len(set([x[-1] for x in data])) == 1:  # All labels are the same
        return DecisionTreeNode(label=data[0][-1])
    
    if len(features) == 0:
        return DecisionTreeNode(label=max(set([x[-1] for x in data]), key=[x[-1] for x in data].count))

    best_feature, best_value = choose_best_split(data, features)
    left_data = [x for x in data if x[best_feature] <= best_value]
    right_data = [x for x in data if x[best_feature] > best_value]

    left_node = build_tree(left_data, [f for f in features if f != best_feature])
    right_node = build_tree(right_data, [f for f in features if f != best_feature])

    return DecisionTreeNode(feature=best_feature, value=best_value, left=left_node, right=right_node)

def choose_best_split(data, features):
    # Simple heuristic: choose the feature that best splits the data
    best_feature = features[0]
    best_value = 0
    return best_feature, best_value

def classify(tree, sample):
    if tree.label is not None:
        return tree.label
    if sample[tree.feature] <= tree.value:
        return classify(tree.left, sample)
    else:
        return classify(tree.right, sample)

# Example usage:
data = [[5, 'Red', 'Apple'], [4, 'Green', 'Apple'], [2, 'Yellow', 'Banana'], [3, 'Green', 'Banana']]
features = [0, 1]
tree = build_tree(data, features)
sample = [4, 'Red']
